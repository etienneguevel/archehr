{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = Path(os.getcwd())\n",
    "DATA_PATH = HERE.parent / 'data' / '1.1' / 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I had severe abdomen pain and was hospitalised for 15 days in ICU, diagnoised with CBD sludge thereafter on udiliv. Doctor advised for ERCP. My question is if the sludge was there does not the medication help in flushing it out? Whether ERCP was the only cure?\n",
      "        \n",
      "\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse(DATA_PATH / 'archehr-qa.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for c in root.findall('case'):\n",
    "    print(c.find('patient_narrative').text)\n",
    "    print(c.find('patient_question').text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(DATA_PATH / 'archehr-qa_key.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = []\n",
    "\n",
    "for c, label in zip(root.findall('case'), data, strict=True):\n",
    "    data_pairs.append(\n",
    "        {\n",
    "            'narrative': c.find('patient_narrative').text,\n",
    "            'patient_question': c.find('patient_question').find('phrase').text,\n",
    "            'clinician_question': c.find('clinician_question').text,\n",
    "            'sentences': [\n",
    "                (i, sentence.text, answer['relevance'])\n",
    "                for i, (sentence, answer) in enumerate(\n",
    "                    zip(c.find('note_excerpt_sentences').findall('sentence'), label['answers'], strict=True)\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'not-relevant': 239, 'essential': 138, 'supplementary': 51})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Look at the distribution of phrases importance\n",
    "\n",
    "l = [a for c in data_pairs for (_, _, a) in c['sentences']]\n",
    "Counter(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Cross-encoder\n",
    "\n",
    "cross-encoders are mdels that take both the query and the phrase as inputs and return an embedding.  \n",
    "Here we will use the `MS Marco` model (popular on [huggingface](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_model_name = 'cross-encoder/ms-marco-MiniLM-L6-v2'\n",
    "model = CrossEncoder(cross_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode queries for first case\n",
    "c = data_pairs[0]\n",
    "queries = [c['narrative'], c['patient_question'], c['clinician_question']]\n",
    "sentences = [s for (_, s, _) in c['sentences']]\n",
    "\n",
    "# Make the pairs\n",
    "questions = [\n",
    "    (q, d) for q in queries for d in sentences \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test deberta model\n",
    "\n",
    "The deBerta model ([here](https://huggingface.co/cross-encoder/nli-deberta-v3-base))\n",
    "gives for a sentence pair the corresponding labels: contradiction, entailment, neutral.  \n",
    "\n",
    "For our case we can just convert those labels to non-essential, essential, supplementary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etienneguevel/Documents/SCAI/projects/archehr/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/etienneguevel/Documents/SCAI/projects/archehr/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_model_name = 'cross-encoder/nli-deberta-v3-base'\n",
    "model = CrossEncoder(cross_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('activation_fn', Identity())"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_modules())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  CrossEncoder(\n",
       "    (model): DebertaV2ForSequenceClassification(\n",
       "      (deberta): DebertaV2Model(\n",
       "        (embeddings): DebertaV2Embeddings(\n",
       "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): DebertaV2Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x DebertaV2Layer(\n",
       "              (attention): DebertaV2Attention(\n",
       "                (self): DisentangledSelfAttention(\n",
       "                  (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): DebertaV2SelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): DebertaV2Intermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DebertaV2Output(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (rel_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pooler): ContextPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (activation_fn): Identity()\n",
       "  )),\n",
       " ('model',\n",
       "  DebertaV2ForSequenceClassification(\n",
       "    (deberta): DebertaV2Model(\n",
       "      (embeddings): DebertaV2Embeddings(\n",
       "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): DebertaV2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rel_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): ContextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta',\n",
       "  DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.embeddings',\n",
       "  DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.embeddings.word_embeddings',\n",
       "  Embedding(128100, 768, padding_idx=0)),\n",
       " ('model.deberta.embeddings.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.embeddings.dropout', Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder',\n",
       "  DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer',\n",
       "  ModuleList(\n",
       "    (0-11): 12 x DebertaV2Layer(\n",
       "      (attention): DebertaV2Attention(\n",
       "        (self): DisentangledSelfAttention(\n",
       "          (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): DebertaV2SelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): DebertaV2Intermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): DebertaV2Output(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.0.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.0.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.0.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.0.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.0.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.0.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.0.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.0.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.0.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.0.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.0.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.0.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.0.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.1',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.1.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.1.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.1.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.1.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.1.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.1.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.1.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.1.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.1.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.1.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.1.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.1.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.1.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.1.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.1.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.1.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.1.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.1.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.2',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.2.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.2.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.2.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.2.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.2.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.2.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.2.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.2.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.2.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.2.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.2.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.2.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.2.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.2.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.2.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.2.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.2.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.2.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.3',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.3.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.3.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.3.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.3.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.3.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.3.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.3.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.3.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.3.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.3.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.3.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.3.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.3.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.3.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.3.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.3.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.3.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.3.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.4',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.4.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.4.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.4.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.4.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.4.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.4.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.4.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.4.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.4.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.4.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.4.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.4.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.4.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.4.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.4.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.4.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.4.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.4.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.5',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.5.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.5.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.5.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.5.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.5.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.5.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.5.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.5.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.5.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.5.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.5.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.5.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.5.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.5.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.5.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.5.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.5.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.5.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.6',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.6.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.6.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.6.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.6.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.6.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.6.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.6.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.6.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.6.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.6.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.6.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.6.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.6.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.6.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.6.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.6.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.6.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.6.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.7',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.7.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.7.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.7.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.7.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.7.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.7.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.7.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.7.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.7.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.7.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.7.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.7.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.7.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.7.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.7.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.7.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.7.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.7.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.8',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.8.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.8.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.8.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.8.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.8.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.8.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.8.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.8.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.8.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.8.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.8.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.8.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.8.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.8.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.8.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.8.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.8.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.8.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.9',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.9.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.9.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.9.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.9.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.9.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.9.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.9.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.9.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.9.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.9.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.9.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.9.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.9.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.9.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.9.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.9.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.9.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.9.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.10',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.10.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.10.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.10.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.10.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.10.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.10.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.10.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.10.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.10.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.10.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.10.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.10.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.10.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.10.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.10.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.10.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.10.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.10.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.11',\n",
       "  DebertaV2Layer(\n",
       "    (attention): DebertaV2Attention(\n",
       "      (self): DisentangledSelfAttention(\n",
       "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): DebertaV2SelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): DebertaV2Intermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): DebertaV2Output(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.11.attention',\n",
       "  DebertaV2Attention(\n",
       "    (self): DisentangledSelfAttention(\n",
       "      (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): DebertaV2SelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.11.attention.self',\n",
       "  DisentangledSelfAttention(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.11.attention.self.query_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.11.attention.self.key_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.11.attention.self.value_proj',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.11.attention.self.pos_dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.11.attention.self.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.11.attention.output',\n",
       "  DebertaV2SelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.11.attention.output.dense',\n",
       "  Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.11.attention.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.11.attention.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.layer.11.intermediate',\n",
       "  DebertaV2Intermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.11.intermediate.dense',\n",
       "  Linear(in_features=768, out_features=3072, bias=True)),\n",
       " ('model.deberta.encoder.layer.11.intermediate.intermediate_act_fn',\n",
       "  GELUActivation()),\n",
       " ('model.deberta.encoder.layer.11.output',\n",
       "  DebertaV2Output(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )),\n",
       " ('model.deberta.encoder.layer.11.output.dense',\n",
       "  Linear(in_features=3072, out_features=768, bias=True)),\n",
       " ('model.deberta.encoder.layer.11.output.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.deberta.encoder.layer.11.output.dropout',\n",
       "  Dropout(p=0.1, inplace=False)),\n",
       " ('model.deberta.encoder.rel_embeddings', Embedding(512, 768)),\n",
       " ('model.deberta.encoder.LayerNorm',\n",
       "  LayerNorm((768,), eps=1e-07, elementwise_affine=True)),\n",
       " ('model.pooler',\n",
       "  ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )),\n",
       " ('model.pooler.dense', Linear(in_features=768, out_features=768, bias=True)),\n",
       " ('model.pooler.dropout', Dropout(p=0, inplace=False)),\n",
       " ('model.classifier', Linear(in_features=768, out_features=3, bias=True)),\n",
       " ('model.dropout', Dropout(p=0.1, inplace=False))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_modules())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Remove the last layer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m new_model\n",
      "File \u001b[0;32m~/Documents/SCAI/projects/archehr/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:127\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SCAI/projects/archehr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:645\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Add a child module to the current module.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module) \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(module)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a Module subclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule name should be a string. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple is not a Module subclass"
     ]
    }
   ],
   "source": [
    "# Remove the last layer\n",
    "new_model = torch.nn.Sequential(*list(model.named_modules())[:-1])\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "scores = model.predict(questions)\n",
    "\n",
    "label_mapping = ['non-essential', 'essential', 'supplementary']\n",
    "labels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_questions(c):\n",
    "    # Make the queries\n",
    "    queries = [c['narrative'], c['patient_question'], c['clinician_question']]\n",
    "    sentences = [s for (_, s, _) in c['sentences']]\n",
    "\n",
    "    # Make the pairs\n",
    "    questions = [\n",
    "        (q, d) for q in queries for d in sentences \n",
    "    ]\n",
    "\n",
    "    # Get the answers\n",
    "    answers = [r for (_, _, r) in c['sentences']]\n",
    "\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "answers = []\n",
    "\n",
    "for c in data_pairs:\n",
    "    # Make the questions\n",
    "    questions, a = make_questions(c)\n",
    "\n",
    "    # Calculate the scores\n",
    "    s = model.predict(questions)\n",
    "    scores.extend(s)\n",
    "    answers.extend(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_prediction(scores, label_mapping):\n",
    "    scores = np.array(scores)\n",
    "    labels = np.array(\n",
    "        [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n",
    "    ).reshape((len(scores) // 3, 3))\n",
    "    \n",
    "    def pref(row):\n",
    "        if 'essential' in row:\n",
    "            return 'essential'\n",
    "        \n",
    "        elif 'not-relevant' in row:\n",
    "            return 'not-relevant'\n",
    "\n",
    "        else:\n",
    "            return 'supplementary'\n",
    "    \n",
    "    labels = np.apply_along_axis(pref, axis=1, arr=labels)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.3%\n"
     ]
    }
   ],
   "source": [
    "# Get the real values\n",
    "truth = np.array(answers)\n",
    "\n",
    "# Make all the possible permutations\n",
    "cl = set(np.unique(truth))\n",
    "choices = [[a] for a in cl]\n",
    "\n",
    "for i in range(len(cl) - 1):\n",
    "    new_choices = [l + [a] for l in choices for a in (cl - set(l))]\n",
    "    choices = new_choices\n",
    "\n",
    "# For each possible permutation look at the acc\n",
    "choice_scores = []\n",
    "\n",
    "for choice in choices:\n",
    "    labels = make_prediction(scores, choice)\n",
    "\n",
    "    mask = (truth == 'essential')\n",
    "    acc = sum(labels[mask] == truth[mask]) / len(labels[mask])\n",
    "\n",
    "    choice_scores.append((choice, float(acc)))\n",
    "\n",
    "best_choice, acc = sorted(choice_scores, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "labels = make_prediction(scores, best_choice)\n",
    "print(f\"{acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-2.6728544, -2.2185981,  4.880722 ], dtype=float32),\n",
       " array([-2.147493 , -2.4036314,  4.593408 ], dtype=float32),\n",
       " array([-2.4452193, -2.8953137,  5.480044 ], dtype=float32),\n",
       " array([ 2.9081976, -4.7010617,  2.8155947], dtype=float32),\n",
       " array([ 3.8878734, -5.3210015,  2.6661558], dtype=float32)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.6728544, -2.2185981,  4.880722 ],\n",
       "        [-2.147493 , -2.4036314,  4.593408 ],\n",
       "        [-2.4452193, -2.8953137,  5.480044 ]],\n",
       "\n",
       "       [[ 2.9081976, -4.7010617,  2.8155947],\n",
       "        [ 3.8878734, -5.3210015,  2.6661558],\n",
       "        [-1.6935804, -3.488221 ,  5.5374184]],\n",
       "\n",
       "       [[-1.8459907, -2.6642559,  4.602802 ],\n",
       "        [-1.0479984, -3.4409842,  4.8996525],\n",
       "        [ 1.2287819, -4.7787423,  4.574991 ]]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(scores).reshape((len(truth), 3, 3))[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Sentence transformer\n",
    "\n",
    "Embeddings models are useful for RAG models, here we test to embed\n",
    "the narrative / patient_question / clinician_question as well as the\n",
    "sentences of the excerpt.\n",
    "\n",
    "Then we use both embeddings to try to predict the utility of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Encode queries for first case\n",
    "c = data_pairs[0]\n",
    "queries = [c['narrative'], c['patient_question'], c['clinician_question']]\n",
    "documents = [s for (_, s, _) in c['sentences']]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name='query')\n",
    "doc_embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.similarity(query_embeddings, doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a small classification head to see if it grants good results\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        n_layers: int = 3,\n",
    "        hidden_dim: int = 2048,\n",
    "        output_dim: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = self._make_stack(\n",
    "            self.n_layers,\n",
    "            self.in_dim,\n",
    "            self.hidden_dim,\n",
    "            self.output_dim\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_stack(n_layers, in_dim, hidden_dim, output_dim):\n",
    "        layers = OrderedDict()\n",
    "        layers['layer0'] = nn.Linear(in_dim, hidden_dim)\n",
    "        layers['gelu0'] = nn.ReLU()\n",
    "\n",
    "        for i in range(n_layers - 1):\n",
    "            layers[f'layer{i+1}'] = nn.Linear(hidden_dim, hidden_dim)\n",
    "            layers[f'gelu{i+1}'] = nn.ReLU()\n",
    "\n",
    "        layers['final_layer'] = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Train with the clinician question\n",
    "class CustomDataset(Dataset):\n",
    "   \n",
    "    def __init__(self, data, embed_model):\n",
    "        super().__init__()\n",
    "        self.embed_model = embed_model\n",
    "        self.data = self._make_pairs(data, embed_model)\n",
    "        self.translate_dict = {\n",
    "            a: i \n",
    "            for i, a in enumerate(set([lab for _, lab in self.data]))\n",
    "        }\n",
    "\n",
    "    def _make_pairs(self, data, model_name):\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        d = []\n",
    "        for c in data:\n",
    "            query = torch.tensor(model.encode(c['clinician_question'], prompt_name='query'))\n",
    "            phrases = [\n",
    "                (torch.tensor(model.encode(p)), lab) for _, p, lab in c['sentences']\n",
    "            ]\n",
    "\n",
    "            d.extend([(torch.hstack((query, p)), lab) for p, lab in phrases])\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        v, lab = self.data[index]\n",
    "        return v, self.translate_dict[lab]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, target, progress_bar = None):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inp, lab in dataloader:\n",
    "            # calculate the predictions of the model\n",
    "            p = model(inp)\n",
    "            preds.append(p.argmax(dim=1))\n",
    "            labels.append(lab)\n",
    "\n",
    "        labels = torch.concat(labels)\n",
    "        preds = torch.concat(preds)\n",
    "\n",
    "        # find tp & fo\n",
    "        tp = sum((labels == preds)[labels == target])\n",
    "        fp = sum((labels != preds)[preds == target])\n",
    "\n",
    "        # calculate the metrics\n",
    "        acc = (sum(labels == preds) / len(labels)).item()\n",
    "        rec = (tp / sum(labels == target)).item()\n",
    "        ppv = (tp / (tp + fp)).item()\n",
    "\n",
    "    if progress_bar:\n",
    "        progress_bar.set_postfix(acc=f'{acc:.1%}', recall=f'{rec:.1%}', ppv=f'{ppv:.1%}')\n",
    "\n",
    "    return acc, rec, ppv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (layer0): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (gelu0): GELU(approximate='none')\n",
       "    (layer1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gelu1): GELU(approximate='none')\n",
       "    (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gelu2): GELU(approximate='none')\n",
       "    (final_layer): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate everything needed\n",
    "# The data\n",
    "dataset = CustomDataset(data_pairs, model_name)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (layer0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (gelu0): ReLU()\n",
       "    (layer1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (gelu1): ReLU()\n",
       "    (layer2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (gelu2): ReLU()\n",
       "    (final_layer): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model\n",
    "model = MLP(in_dim=2048, hidden_dim=1024, output_dim=len(dataset.translate_dict))\n",
    "\n",
    "# The loss and optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = AdamW(model.parameters())\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [07:46<00:00,  2.14it/s, acc=56.5%, ppv=nan%, recall=0.0%]  \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "accuracy, recall, positive_pred = [], [], [] \n",
    "\n",
    "for i in (progress_bar := tqdm(range(n_epochs))):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "        # calculate the predictions of the model\n",
    "        preds = model(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        l = loss(preds, labels)\n",
    "        \n",
    "        # backpropagation\n",
    "        l.backward()\n",
    "\n",
    "        # optimize parameters\n",
    "        optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        # calculate the metrics\n",
    "        acc, rec, ppv = eval_model(\n",
    "            model,\n",
    "            dataloader,\n",
    "            dataset.translate_dict.get('essential'),\n",
    "            progress_bar\n",
    "        )\n",
    "\n",
    "        accuracy.append(acc)\n",
    "        recall.append(rec)\n",
    "        positive_pred.append(ppv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 14/14 [00:00<00:00, 91.64it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "preds, labels = [], [] \n",
    "\n",
    "for inp, lab in tqdm(dataloader):\n",
    "    p = model(inp)\n",
    "\n",
    "    preds.append(p.argmax(dim=1))\n",
    "    labels.append(lab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
